1) Running parse.pl

- Make sure you have the DBI, Time::Local, Text::CSV and XML::Parser::PerlSAX Perl modules installed.

- Create a MySQL database and perform the following queries:
 CREATE TABLE user (id SERIAL, name VARCHAR(255) UNIQUE);
 CREATE TABLE `edge` (
  `fromuser` int(11) DEFAULT NULL,
  `touser` int(11) DEFAULT NULL,
  `weight` float DEFAULT NULL,
  `article` varchar(255) DEFAULT NULL
 );
 CREATE TABLE `eigenvector` (
  `userid` int(11) NOT NULL,
  `article` varchar(255) NOT NULL,
  `v1` double NOT NULL COMMENT 'vectorelement to the smallest eigenvalue',
  `v2` double NOT NULL COMMENT 'vectorelement to the 2nd smallest eigenvalue',
  PRIMARY KEY (`userid`,`article`)
 );
 CREATE TABLE `eigenvalue` (
  `article` varchar(255) NOT NULL,
  `lambda1` double NOT NULL COMMENT 'smallest eigenvalue',
  `lambda2` double NOT NULL COMMENT 'second smallest eigenvalue',
  PRIMARY KEY (`article`)
 );


- Get a Wikipedia stub file from http://dumps.wikimedia.org/

- Run 'perl parse.pl CONNECTION ARTICLES DTMAX DUMPFILE' with
    CONNECTION : database credentials in the form "dbname,dbhost,dbuser,dbpass"
    ARTICLES : page titles whose history you want to analyse as comma-separated list, e.g. "Alan Smithee, Ang Lee, Aussagenlogik" (use those double quotes!)
    DTMAX : the maximum time difference between edits (as integer)
    DUMPFILE : path to the Wikipedia dump file

- Compile the c++ program "evgen"

- Run 'evgen "Articlename"'

